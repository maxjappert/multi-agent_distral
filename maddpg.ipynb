{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c65a204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "from gym import spaces, Env\n",
    "from gym.utils import seeding\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the environment's elements\n",
    "EMPTY, WALL, TARGET1, TARGET2, AGENT1, AGENT2, SUCCESS, TARGET1_OC2, TARGET2_OC1 = range(9)\n",
    "ACTIONS = [NOOP, UP, DOWN, LEFT, RIGHT] = range(5)\n",
    "\n",
    "# Define colors for rendering\n",
    "COLORS = {\n",
    "    EMPTY: [1.0, 1.0, 1.0],  # White\n",
    "    WALL: [0.0, 0.0, 0.0],  # Black\n",
    "    TARGET1: [0.0, 0.5, 0.0],  # Dark Green\n",
    "    TARGET2: [0.5, 0.0, 0.0],  # Dark Red\n",
    "    AGENT1: [0.5, 1.0, 0.5],  # Light Green\n",
    "    AGENT2: [1.0, 0.5, 0.5],  # Light Red\n",
    "    SUCCESS: [1.0, 0.0, 1.0],  # Pink\n",
    "    TARGET1_OC2: [0.5, 0.5, 0.5],\n",
    "    TARGET2_OC1: [0.5, 0.5, 0.5],\n",
    "}\n",
    "\n",
    "# Define the action to position update mapping\n",
    "ACTION_EFFECTS = {\n",
    "    NOOP: (0, 0),\n",
    "    UP: (-1, 0),\n",
    "    DOWN: (1, 0),\n",
    "    LEFT: (0, -1),\n",
    "    RIGHT: (0, 1),\n",
    "}\n",
    "\n",
    "\n",
    "class GridworldEnv(Env):\n",
    "    metadata = {'render.modes': ['human', 'rgb_array']}\n",
    "\n",
    "    def __init__(self, plan, from_file=True):\n",
    "        super(GridworldEnv, self).__init__()\n",
    "        self.action_space = {0: spaces.Discrete(5), 1: spaces.Discrete(5)}\n",
    "        self.action_combinations = list(itertools.product(ACTIONS, repeat=2))\n",
    "\n",
    "        # Load the grid map\n",
    "        if from_file:\n",
    "            this_file_path = os.path.dirname(os.path.realpath(__file__))\n",
    "            grid_map_path = os.path.join(this_file_path, 'tasks/task{}.txt'.format(plan))\n",
    "            self.start_grid_map = self._read_grid_map(grid_map_path)\n",
    "        else:\n",
    "            self.start_grid_map = np.array(plan, dtype=int)\n",
    "\n",
    "        self.current_grid_map = np.copy(self.start_grid_map)\n",
    "        self.observation_space = spaces.Box(low=0, high=max(EMPTY, WALL, TARGET1, TARGET2, AGENT1, AGENT2, SUCCESS),\n",
    "                                            shape=self.current_grid_map.shape, dtype=int)\n",
    "\n",
    "        # Initialize agents' states\n",
    "        self.agents_start_coords, self.agents_target_coords = self._find_agents_and_targets()\n",
    "        self.current_agents_coords = np.copy(self.agents_start_coords)\n",
    "        self.move_completed = [False, False]\n",
    "        self.episode_total_reward = 0.0\n",
    "        self.viewer = None\n",
    "        self.seed()\n",
    "\n",
    "        # Env state: [agent 1 x position (this is actually vertical axis, sorry abt naming), agent 1 y position (this is actually horizontal axis), agent 2 x position, agent 2 y position, agent 1 previous action, flag on whether player 1 has reached goal – 1 if true, agent 2 previous action, flag on whether player 2 has reached goal – 1 if true]\n",
    "        self.current_game_state = [self.current_agents_coords[0][0], self.current_agents_coords[0][1], self.current_agents_coords[1][0], self.current_agents_coords[1][1], NOOP, 0, NOOP, 0]\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def get_state_single(self, state, agent_idx):\n",
    "        \"\"\"\n",
    "        Returns the state for the specified agent from the combined state.\n",
    "        \"\"\"\n",
    "        # state of env has coordinates p1 +coordinates p2 + p1 prev action +  p1 reward so far + p2 prev action  + p2 reward so far\n",
    "        # state of each agent should be coordinates p1 + coordinates p2 + their own prev action  + other agent's reward so far\n",
    "        if agent_idx==0:\n",
    "            return state[:5]+[state[7]]\n",
    "        else:\n",
    "            return state[:4]+[state[6]]+[state[5]]\n",
    "\n",
    "    def get_legal_action_pairs(self):\n",
    "        legal_action_pairs = []\n",
    "\n",
    "        for comb in self.action_combinations:\n",
    "            if self.move_legal(comb):\n",
    "                legal_action_pairs.append(comb)\n",
    "\n",
    "        return legal_action_pairs\n",
    "\n",
    "    def move_legal(self, action_pair):\n",
    "\n",
    "        # If an agent has reached their goal, they can only remain where they are\n",
    "        if self.move_completed[0] == True and action_pair[0] != NOOP:\n",
    "            return False\n",
    "\n",
    "        if self.move_completed[1] == True and action_pair[1] != NOOP:\n",
    "            return False\n",
    "\n",
    "        dy_1, dx_1 = ACTION_EFFECTS[action_pair[0]]\n",
    "        dy_2, dx_2 = ACTION_EFFECTS[action_pair[1]]\n",
    "        y_1, x_1 = self.current_agents_coords[0][0] + dy_1, self.current_agents_coords[0][1] + dx_1\n",
    "        y_2, x_2 = self.current_agents_coords[1][0] + dy_2, self.current_agents_coords[1][1] + dx_2\n",
    "\n",
    "        # If the agents switch positions that's illegal\n",
    "        if ((self.current_agents_coords[0][0] == y_2 and self.current_agents_coords[0][1] == x_2)\n",
    "                or (self.current_agents_coords[1][0] == y_1 and self.current_agents_coords[1][1] == x_1)):\n",
    "            return False\n",
    "\n",
    "        return (self._within_bounds(y_1, x_1) and self._within_bounds(y_2, x_2)\n",
    "                and self.current_grid_map[y_1, x_1] != WALL and self.current_grid_map[y_2, x_2] != WALL\n",
    "                and (y_1, x_1) != (y_2, x_2))\n",
    "\n",
    "    def step(self, actions):\n",
    "        rewards = [0.0, 0.0]\n",
    "        new_agent_coords = np.copy(self.current_agents_coords)\n",
    "\n",
    "        if type(actions) is not tuple and len(actions) != 2:\n",
    "            print(actions)\n",
    "\n",
    "        for agent_idx, action in enumerate(actions):\n",
    "            if self.move_completed[agent_idx]:\n",
    "                continue\n",
    "\n",
    "            dy, dx = ACTION_EFFECTS[action]\n",
    "            y, x = self.current_agents_coords[agent_idx]\n",
    "            new_y, new_x = y + dy, x + dx\n",
    "\n",
    "            # Check for illegal moves\n",
    "            if not self.move_legal(actions):\n",
    "                rewards[agent_idx] = -0.1\n",
    "                continue\n",
    "\n",
    "            if self._target_reached(agent_idx, new_y, new_x):\n",
    "                self.move_completed[agent_idx] = True\n",
    "                rewards[agent_idx] = 100.0\n",
    "            else:\n",
    "                rewards[agent_idx] = -0.1\n",
    "\n",
    "            new_agent_coords[agent_idx] = [new_y, new_x]\n",
    "\n",
    "        self._update_grid_map(self.current_agents_coords, new_agent_coords)\n",
    "        self.current_agents_coords = new_agent_coords\n",
    "\n",
    "        self.current_game_state = [self.current_agents_coords[0][0], self.current_agents_coords[0][1], self.current_agents_coords[1][0], self.current_agents_coords[1][1], actions[0], int(self.move_completed[0]), actions[1], int(self.move_completed[1])]\n",
    "\n",
    "        # Update game state and check if episode is done\n",
    "        done = all(self.move_completed)\n",
    "        return self.current_game_state, rewards, self.move_completed\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_grid_map = np.copy(self.start_grid_map)\n",
    "        self.current_agents_coords = np.copy(self.agents_start_coords)\n",
    "        self.move_completed = [False, False]\n",
    "        self.episode_total_reward = 0.0\n",
    "        self.current_game_state = [self.current_agents_coords[0][0], self.current_agents_coords[0][1], self.current_agents_coords[1][0], self.current_agents_coords[1][1], NOOP, 0, NOOP, 0]\n",
    "        return self.current_game_state\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        img = self._gridmap_to_image()\n",
    "        if mode == 'rgb_array':\n",
    "            return img\n",
    "        elif mode == 'human':\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "\n",
    "    def _within_bounds(self, y, x):\n",
    "        return 0 <= y < self.current_grid_map.shape[0] and 0 <= x < self.current_grid_map.shape[1]\n",
    "\n",
    "    def _target_reached(self, agent_idx, y, x):\n",
    "        target = TARGET1 if agent_idx == 0 else TARGET2\n",
    "        return self.current_grid_map[y, x] == target\n",
    "\n",
    "    def _find_agents_and_targets(self):\n",
    "        start_coords = []\n",
    "        target_coords = []\n",
    "        for agent, target in [(AGENT1, TARGET1), (AGENT2, TARGET2)]:\n",
    "            sy, sx = np.where(self.start_grid_map == agent)\n",
    "            ty, tx = np.where(self.start_grid_map == target)\n",
    "            start_coords.append([sy[0], sx[0]])\n",
    "            target_coords.append([ty[0], tx[0]])\n",
    "        return np.array(start_coords), np.array(target_coords)\n",
    "\n",
    "    def _update_grid_map(self, old_coords, new_coords):\n",
    "\n",
    "        old_grid_map = copy.deepcopy(self.current_grid_map)\n",
    "\n",
    "        for (y, x) in old_coords:\n",
    "            self.current_grid_map[y, x] = EMPTY\n",
    "\n",
    "        for idx, (y, x) in enumerate(new_coords):\n",
    "            self.current_grid_map[y, x] = AGENT1 if idx == 0 else AGENT2\n",
    "\n",
    "        if new_coords[0][0] == self.agents_target_coords[0][0] and new_coords[0][1] == self.agents_target_coords[0][1]:\n",
    "            self.current_grid_map[new_coords[0][0], new_coords[0][1]] = SUCCESS\n",
    "        if new_coords[1][0] == self.agents_target_coords[1][0] and new_coords[1][1] == self.agents_target_coords[1][1]:\n",
    "            self.current_grid_map[new_coords[1][0], new_coords[1][1]] = SUCCESS\n",
    "\n",
    "        # In case an agent is on the other agent's target\n",
    "        for i, (y, x) in enumerate(new_coords):\n",
    "            if self.current_grid_map[y, x] == AGENT1 and (old_grid_map[y, x] == TARGET2 or old_grid_map[y, x] == TARGET2_OC1):\n",
    "                self.current_grid_map[y, x] = TARGET2_OC1\n",
    "\n",
    "            if self.current_grid_map[y, x] == AGENT2 and (old_grid_map[y, x] == TARGET1 or old_grid_map[y, x] == TARGET1_OC2):\n",
    "                self.current_grid_map[y, x] = TARGET1_OC2\n",
    "\n",
    "        # Check if agent moved away from occupying the foreign target\n",
    "        if old_grid_map[old_coords[1][0], old_coords[1][1]] == TARGET1_OC2 and (new_coords[1][0] != old_coords[1][0] or new_coords[1][1] != old_coords[1][1]):\n",
    "            self.current_grid_map[old_coords[1][0], old_coords[1][1]] = TARGET1\n",
    "\n",
    "        if old_grid_map[old_coords[0][0], old_coords[0][1]] == TARGET2_OC1 and (new_coords[0][0] != old_coords[0][0] or new_coords[0][1] != old_coords[0][1]):\n",
    "            self.current_grid_map[old_coords[0][0], old_coords[0][1]] = TARGET2\n",
    "\n",
    "    def _gridmap_to_image(self):\n",
    "        img = np.zeros((*self.current_grid_map.shape, 3))\n",
    "        for i in range(self.current_grid_map.shape[0]):\n",
    "            for j in range(self.current_grid_map.shape[1]):\n",
    "                img[i, j] = COLORS[self.current_grid_map[i, j]]\n",
    "        return img\n",
    "\n",
    "    def _read_grid_map(self, grid_map_path):\n",
    "        with open(grid_map_path, 'r') as file:\n",
    "            grid_map = [[int(cell) for cell in line.split()] for line in file]\n",
    "        return np.array(grid_map, dtype=int)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f522a853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from gridworld_env import GridworldEnv\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.tanh(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.tanh(x)\n",
    "        return x\n",
    "\n",
    "# class CriticNetwork(nn.Module):\n",
    "#     def __init__(self, state_dim, action_dim):\n",
    "#         super(CriticNetwork, self).__init__()\n",
    "#         self.fc1 = nn.Linear(state_dim + action_dim, 64)\n",
    "#         self.fc2 = nn.Linear(64, 64)\n",
    "#         self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "#     def forward(self, state, action):\n",
    "#         x = torch.cat([state, action], dim=1)\n",
    "#         x = self.fc1(x)\n",
    "#         x = torch.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = torch.relu(x)\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MADDPGAgent:\n",
    "    def __init__(self, env, state_dim, action_dim, agent_id, lr_actor, lr_critic, gamma, tau):\n",
    "        self.agent_id = agent_id\n",
    "        self.env = env\n",
    "        self.actor = ActorNetwork(state_dim, action_dim)\n",
    "        self.critic = CriticNetwork(state_dim, action_dim)\n",
    "        self.actor_target = ActorNetwork(state_dim, action_dim)\n",
    "        self.critic_target = CriticNetwork(state_dim, action_dim)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        # Initialize target networks\n",
    "        self.soft_update(self.actor, self.actor_target, 1.0)\n",
    "        self.soft_update(self.critic, self.critic_target, 1.0)\n",
    "\n",
    "    def act(self, state, epsilon=0.0):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return self.env.action_space[self.agent_id].sample()\n",
    "        else:\n",
    "            state = torch.FloatTensor(state)\n",
    "            action = self.actor(state)\n",
    "            return action.detach().numpy()\n",
    "\n",
    "    def update(self, batch, other_agents):\n",
    "        state = torch.FloatTensor(batch['state'])\n",
    "        action = torch.FloatTensor(batch['action'])\n",
    "        reward = torch.FloatTensor(batch['reward'])\n",
    "        next_state = torch.FloatTensor(batch['next_state'])\n",
    "        done = torch.FloatTensor(batch['done'])\n",
    "\n",
    "        # Update critic network\n",
    "        next_actions = torch.cat([agent.actor_target(next_state[:, i*state_dim:(i+1)*state_dim]) for i, agent in enumerate(other_agents)], dim=1)\n",
    "        target_q = self.critic_target(next_state, next_actions)\n",
    "        expected_q = reward + self.gamma * (1 - done) * target_q\n",
    "        critic_loss = nn.MSELoss()(self.critic(state, action), expected_q.detach())\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Update actor network\n",
    "        my_action = self.actor(state[:, self.agent_id*state_dim:(self.agent_id+1)*state_dim])\n",
    "        actor_loss = -self.critic(state, torch.cat([agent.actor(state[:, i*state_dim:(i+1)*state_dim]) for i, agent in enumerate(other_agents)], dim=1))\n",
    "        actor_loss = actor_loss.mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.soft_update(self.actor, self.actor_target, self.tau)\n",
    "        self.soft_update(self.critic, self.critic_target, self.tau)\n",
    "\n",
    "    def soft_update(self, source, target, tau):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "class MADDPGTrainer:\n",
    "    def __init__(self, env, state_dim, action_dim, lr_actor, lr_critic, gamma, tau, buffer_size, batch_size):\n",
    "        self.env = env\n",
    "        self.agents = [MADDPGAgent(env, state_dim, action_dim, i, lr_actor, lr_critic, gamma, tau) for i in range(2)]\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train(self, num_episodes, max_steps, epsilon_start, epsilon_end, epsilon_decay):\n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            epsilon = max(epsilon_end, epsilon_start * (epsilon_decay ** episode))\n",
    "            done = [False] * 2\n",
    "\n",
    "            for step in range(max_steps):\n",
    "                actions = [agent.act(state[i], epsilon) for i, agent in enumerate(self.agents)]\n",
    "                next_state, rewards, done = self.env.step(actions)\n",
    "                self.replay_buffer.add(state, actions, rewards, next_state, done)\n",
    "\n",
    "                if all(done):\n",
    "                    break\n",
    "\n",
    "                batch = self.replay_buffer.sample(self.batch_size)\n",
    "                for agent in self.agents:\n",
    "                    agent.update(batch, [self.agents[i] for i in range(2) if i != agent.agent_id])\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            self.env.render(sum(rewards), done[0], f'episode_{episode}.png')\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = {'state': [], 'action': [], 'reward': [], 'next_state': [], 'done': []}\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer['state']) >= self.buffer_size:\n",
    "            self.buffer['state'].pop(0)\n",
    "            self.buffer['action'].pop(0)\n",
    "            self.buffer['reward'].pop(0)\n",
    "            self.buffer['next_state'].pop(0)\n",
    "            self.buffer['done'].pop(0)\n",
    "\n",
    "        self.buffer['state'].append(state)\n",
    "        self.buffer['action'].append(action)\n",
    "        self.buffer['reward'].append(reward)\n",
    "        self.buffer['next_state'].append(next_state)\n",
    "        self.buffer['done'].append(done)\n",
    "\n",
    "    # def sample(self, batch_size):\n",
    "    #     indices = np.random.choice(len(self.buffer['state']), size=batch_size, replace=False)\n",
    "    #     batch = {key: [self.buffer[key][i] for i in indices] for key in self.buffer}\n",
    "    #     return batch\n",
    "    def sample(self, batch_size):\n",
    "        if len(self.buffer['state']) < batch_size:\n",
    "            indices = np.random.choice(len(self.buffer['state']), size=batch_size, replace=True)\n",
    "        else:\n",
    "            indices = np.random.choice(len(self.buffer['state']), size=batch_size, replace=False)\n",
    "        batch = {key: [self.buffer[key][i] for i in indices] for key in self.buffer}\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e49adcbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x10 and 13x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m num_agents \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     25\u001b[0m trainer \u001b[38;5;241m=\u001b[39m MADDPGTrainer(env, state_dim, action_dim, lr_actor, lr_critic, gamma, tau, buffer_size, batch_size)\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_decay\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mMADDPGTrainer.train\u001b[0;34m(self, num_episodes, max_steps, epsilon_start, epsilon_end, epsilon_decay)\u001b[0m\n\u001b[1;32m    135\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents:\n\u001b[0;32m--> 137\u001b[0m         \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magents\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;28msum\u001b[39m(rewards), done[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mMADDPGAgent.update\u001b[0;34m(self, batch, other_agents)\u001b[0m\n\u001b[1;32m     92\u001b[0m target_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_target(next_state, next_actions)\n\u001b[1;32m     93\u001b[0m expected_q \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m done) \u001b[38;5;241m*\u001b[39m target_q\n\u001b[0;32m---> 94\u001b[0m critic_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m, expected_q\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     96\u001b[0m critic_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mCriticNetwork.forward\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action):\n\u001b[1;32m     49\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([state, action], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     52\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x10 and 13x64)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from gridworld_env import GridworldEnv\n",
    "lr_actor = 1e-4\n",
    "lr_critic = 1e-3\n",
    "gamma = 0.99\n",
    "tau = 0.01\n",
    "buffer_size = 10000\n",
    "batch_size = 64\n",
    "num_episodes = 1000\n",
    "max_steps = 300\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.1\n",
    "epsilon_decay = 0.995\n",
    "NUM_GAMES = 50\n",
    "TURN_LIMIT = 200\n",
    "IS_MONITOR = True\n",
    "env = GridworldEnv('6')\n",
    "env.reset()\n",
    "state_dim = 8\n",
    "action_dim = 5\n",
    "\n",
    "num_agents = 2\n",
    "\n",
    "trainer = MADDPGTrainer(env, state_dim, action_dim, lr_actor, lr_critic, gamma, tau, buffer_size, batch_size)\n",
    "trainer.train(num_episodes, max_steps, epsilon_start, epsilon_end, epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bbd42e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
